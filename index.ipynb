{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad40542-40fd-47cd-8a7f-4c7c95301189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086c5b9b-edd5-43c1-a9dc-01a9d6b99c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a8da0b-77c9-48a1-a13b-b0596b3c491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f9e4fa-f931-4b17-b73a-877f96941011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c97c36-03ac-480d-be91-c81e3478ef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b33297-ed7a-4ad9-a1ac-bbb7a5e72d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=2), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=2), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896c1b9c-71bc-4c41-ada1-5dbc0f44b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f1511f-8144-4942-8ffc-ed60e31b6618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(results.left_hand_landmarks.landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fefd8c-5b76-486c-8b0e-421fd5704afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf61931-06be-4275-8b72-17e6c198d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_landmarks(frame, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437d8f4b-17d2-4a5b-8e77-ffe06144aca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83bc77c-be9e-47df-9dfb-f585588d5ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extreact key points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0311a61-6611-42ec-9bd7-4e7c7c989955",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.pose_landmarks.landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44b033a-fe0c-47e7-a4a3-d83a49a461fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = []\n",
    "for res in results.pose_landmarks.landmark:\n",
    "    test = np.array([res.x, res.y, res.z, res.visibility])\n",
    "    pose.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b13f6-b0e0-48c5-969b-8fc60492db78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7898f8-372c-4164-94a6-7acc09916330",
   "metadata": {},
   "outputs": [],
   "source": [
    "face = np.zeros(1404)  # Default value in case no face landmarks are detected\n",
    "if results.face_landmarks:\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d4d7e5-784d-45ef-8944-a211c11c1955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ac2411-8d8a-4467-b380-b7d4d65475dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = extract_keypoints(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1278c71b-3652-47a8-b00d-d51617455cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2e9dd0-2cf7-40c3-a3de-9d1bf4d73ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('0', result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49447d9d-1883-410e-b536-62bc10dc7c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load('0.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92b413c-233c-4e1a-91cb-db0c4c3c7f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data') \n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['iam', 'good', 'boy'])\n",
    "\n",
    "# Number of sequences per action\n",
    "no_sequences = 30\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "# Folder start index\n",
    "start_folder = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e2dbf2-c153-4706-bec0-393657775224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(DATA_PATH, exist_ok=True)\n",
    "\n",
    "# for action in actions:\n",
    "#     action_path = os.path.join(DATA_PATH, action)\n",
    "\n",
    "#     # Ensure the action directory exists before listing contents\n",
    "#     os.makedirs(action_path, exist_ok=True)\n",
    "\n",
    "#     # Get existing sequence folders and find the max index safely\n",
    "#     existing_folders = []\n",
    "#     if os.path.exists(action_path):  # Check if the directory exists\n",
    "#         existing_folders = [int(folder) for folder in os.listdir(action_path) if folder.isdigit()]\n",
    "    \n",
    "#     dirmax = max(existing_folders) if existing_folders else 0  # Set to 0 if empty\n",
    "\n",
    "#     # Create new sequence folders\n",
    "#     for sequence in range(1, no_sequences + 1):\n",
    "#         new_folder = os.path.join(action_path, str(dirmax + sequence))\n",
    "#         os.makedirs(new_folder, exist_ok=True)  # Use exist_ok=True to avoid errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8357f1ea-b09b-49a8-83c7-3541d4162df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize Video Capture\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "#     for action in actions:\n",
    "#         for sequence in range(start_folder, start_folder + no_sequences):\n",
    "#             for frame_num in range(sequence_length):\n",
    "\n",
    "#                 # Read feed\n",
    "#                 ret, frame = cap.read()\n",
    "\n",
    "#                 # Check if frame is captured\n",
    "#                 if not ret:\n",
    "#                     print(\"⚠️ Warning: Unable to read frame. Skipping...\")\n",
    "#                     continue\n",
    "\n",
    "#                 # Make detections\n",
    "#                 image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "#                 results = holistic.process(image)\n",
    "\n",
    "#                 # Convert back to BGR for OpenCV display\n",
    "#                 image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "#                 # Draw landmarks\n",
    "#                 if results.face_landmarks:\n",
    "#                     mp.solutions.drawing_utils.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION)\n",
    "#                 if results.pose_landmarks:\n",
    "#                     mp.solutions.drawing_utils.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "#                 if results.left_hand_landmarks:\n",
    "#                     mp.solutions.drawing_utils.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "#                 if results.right_hand_landmarks:\n",
    "#                     mp.solutions.drawing_utils.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "#                 # NEW Apply wait logic\n",
    "#                 if frame_num == 0:\n",
    "#                     cv2.putText(image, 'STARTING COLLECTION', (120, 200),\n",
    "#                                 cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "#                     cv2.putText(image, f'Collecting frames for {action} Video Number {sequence}', (15, 12),\n",
    "#                                 cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "#                     cv2.imshow('OpenCV Feed', image)\n",
    "#                     cv2.waitKey(5000)\n",
    "#                 else:\n",
    "#                     cv2.putText(image, f'Collecting frames for {action} Video Number {sequence}', (15, 12),\n",
    "#                                 cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "#                     cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "#                 # NEW Ensure directory exists before saving\n",
    "#                 npy_folder = os.path.join(DATA_PATH, action, str(sequence))\n",
    "#                 os.makedirs(npy_folder, exist_ok=True)  # ✅ Create missing directory\n",
    "                \n",
    "#                 # Extract keypoints function\n",
    "#                 def extract_keypoints(results):\n",
    "#                     pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "#                     face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "#                     lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(63)\n",
    "#                     rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(63)\n",
    "#                     return np.concatenate([pose, face, lh, rh])\n",
    "\n",
    "#                 # Export keypoints\n",
    "#                 keypoints = extract_keypoints(results)\n",
    "#                 npy_path = os.path.join(npy_folder, f\"{frame_num}.npy\")\n",
    "#                 np.save(npy_path, keypoints)  # ✅ Save keypoints\n",
    "\n",
    "#                 # Break gracefully\n",
    "#                 if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#                     break\n",
    "\n",
    "# # Release webcam and close OpenCV windows\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e882312-f51a-456b-93c0-f1fd6c1e94b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f554fb3-10b8-4fab-8a11-6172c7dfbd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pREPROCESS DATA AND CREATE LABELS AND FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3bf415-341a-4d52-8ecd-9432a0c92a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834ec057-8dc6-4ea4-a452-013e0070c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab10e07-38a2-4c1f-abae-e54f96450969",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce741fb7-a3f1-4cb6-9bbe-02a4735f1fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "\n",
    "for action in actions:\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):  # ✅ FIXED: Removed +1 to avoid extra frame\n",
    "            npy_path = os.path.join(DATA_PATH, action, str(sequence), f\"{frame_num}.npy\")\n",
    "            \n",
    "            # ✅ FIX: Check if file exists before loading\n",
    "            if os.path.exists(npy_path):\n",
    "                res = np.load(npy_path)\n",
    "                window.append(res)\n",
    "            else:\n",
    "                print(f\"⚠️ Warning: Missing file {npy_path}, using zeros as placeholder.\")\n",
    "                window.append(np.zeros(1662))  # Adjust based on your keypoint size\n",
    "\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])  # Ensure label_map exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ca9547-15b2-464d-864f-2c909023bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5437f38a-c4ef-402b-867c-a40e9905b84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e90954a-96a0-4adc-a296-f99402303102",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9371628-4bda-41d8-8ab0-f5c33ec717d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f932a358-0d00-45ab-912d-cdc744e90fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9250b14-6880-4cfb-8d28-c2f25cbc8b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd94b53f-0059-46d5-b831-406454d06df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3964e36-18ac-47bb-8e8e-81cad16b06a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build and train neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe9de42-722c-4b8a-9165-fb0bd4cbeece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential #sequential flow of neural network creation\n",
    "from tensorflow.keras.layers import LSTM, Dense #a component to build the neural network\n",
    "from tensorflow.keras.callbacks import TensorBoard #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6178dbda-b295-413f-88b2-a5cdd52ed69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef8126d-0e4c-4b7e-9da2-41c6bba9ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "\n",
    "# Define the model\n",
    "# model = Sequential([\n",
    "#     Input(shape=(30, 1662)),  # Define input shape here\n",
    "#     LSTM(64, return_sequences=True, activation='relu'),\n",
    "#     LSTM(128, return_sequences=True, activation='relu'),\n",
    "#     LSTM(64, return_sequences=False, activation='relu'),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(32, activation='relu'),\n",
    "#     Dense(actions.shape[0], activation='softmax')  # Ensure actions is defined\n",
    "# ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91418b6e-9e7a-4b95-9a9f-341bea14a71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd52b44-12b1-461e-a8c2-11f8952d846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=2000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b1fc70-72da-47b5-8afe-cf6715d87947",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d6412e-325b-4267-878a-a76294b0e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAke predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4a6c2a-d7bd-4893-bfb4-66aa55a50a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e8f6e9-2cb9-48d1-8b79-88f886269948",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(res) > 4:\n",
    "    predicted_action = actions[np.argmax(res[4])]\n",
    "else:\n",
    "    predicted_action = actions[np.argmax(res[0])]  \n",
    "\n",
    "if len(y_test) > 4:\n",
    "    actual_action = actions[np.argmax(y_test[4])]\n",
    "else:\n",
    "    actual_action = actions[np.argmax(y_test[0])]\n",
    "\n",
    "print(\"Predicted Action:\", predicted_action)\n",
    "print(\"Actual Action:\", actual_action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376ab60b-507c-49db-8543-b9748783d930",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('action.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0563647a-ce76-4519-8964-29f9a1fb6b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495dfea8-68fa-4f39-a1b5-15fc1d9034d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('action.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9110d39d-1ec7-4b59-b230-0db1e921b2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe0447c-5d31-422c-bf80-03ca47d77991",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ff9b2c-ab33-46c7-b442-597a83e00031",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914861b2-54b2-4f40-ba17-dcaed3700d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9adfd8c-2c23-44eb-b01e-6f8ea9d8b01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15813d4d-7ef6-4025-a2f6-ec6bf98dfc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1663ffae-47c1-405c-aa71-15c30a5a4957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors = [(245,117,16), (117,245,16), (16,117,245)]# Generate one color per action\n",
    "\n",
    "# def prob_viz(res, actions, input_frame, colors):\n",
    "#     output_frame = input_frame.copy()\n",
    "\n",
    "#     # Debugging prints\n",
    "#     print(f\"res length: {len(res)}, actions length: {len(actions)}, colors length: {len(colors)}\")\n",
    "\n",
    "#     for num, prob in enumerate(res):\n",
    "#         if num >= len(actions) or num >= len(colors):  # Prevent index error\n",
    "#             print(f\"Skipping index {num}, out of range\")\n",
    "#             continue\n",
    "            \n",
    "#         cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "#         cv2.putText(output_frame, actions[num], (0, 85 + num * 40), \n",
    "#                     cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "#     return output_frame\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = [(245, 117, 16), (117, 245, 16), (16, 117, 245)]  # Generate one color per action\n",
    "\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "\n",
    "    # Debugging prints\n",
    "    print(f\"res length: {len(res)}, actions length: {len(actions)}, colors length: {len(colors)}\")\n",
    "    print(f\"res values: {res}\")\n",
    "\n",
    "    for num, prob in enumerate(res):\n",
    "        if num >= len(actions) or num >= len(colors):  # Prevent index error\n",
    "            print(f\"Skipping index {num}, out of range\")\n",
    "            continue\n",
    "\n",
    "        # Ensure prob is a scalar by taking the first value if it's an array\n",
    "        if isinstance(prob, (np.ndarray, list)):\n",
    "            prob = float(prob[0])  # Convert to a single float value\n",
    "        \n",
    "        # Draw probability visualization\n",
    "        cv2.rectangle(output_frame, (0, 60 + num * 40), (int(prob * 100), 90 + num * 40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85 + num * 40), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b45760-fdff-4219-98b5-cfafd6a31271",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "plt.imshow(prob_viz(res, actions, image, colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f673eb5-7f36-4862-921c-e0e82ff729f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.85\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "            # 3. Viz logic\n",
    "            if len(predictions) >= 10 and np.unique(predictions[-10:])[0] == np.argmax(res): \n",
    "                if res[np.argmax(res)] > threshold: \n",
    "                    if len(sentence) > 0:\n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5:\n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "        \n",
    "        cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11d435c-e802-40ee-bbc4-cdb412a1c37b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9aac57-ca4b-4a81-9616-857bb0dafb79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe1c790-108c-40f9-b105-f17bf3a76c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
